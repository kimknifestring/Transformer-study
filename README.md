# 📖 My Transformer Study Record

이 저장소는 트랜스포머(Transformer) 언어 모델을 구현하며 학습한 내용을 기록한 프로젝트입니다.

단순한 Bigram 모델에서 시작하여, 현대 언어 모델의 핵심인 트랜스포머 아키텍처를 **사전 훈련된 BPE 토크나이저**와 함께 직접 구현하는 전체 과정을 담고 있습니다.

---

## 🔥 주요 학습 및 구현 내용

- **트랜스포머 아키텍처 직접 구현**: PyTorch를 사용하여 `nn.Module` 기반으로 트랜스포머의 모든 핵심 부품을 밑바닥부터 구현했습니다.
  - **셀프 어텐션**: Scaled Dot-Product Attention의 원리를 이해하고 Query, Key, Value를 사용한 정보 교환 메커니즘을 구현.
  - **멀티 헤드 어텐션**: 여러 관점에서 문맥을 동시에 파악하는 Multi-Head Attention 구현.
  - **피드 포워드 네트워크**: 어텐션으로 취합된 정보를 처리하는 Feed-Forward (FFN) 계층 구현.
  - **안정화 기법**: 모델을 깊게 쌓기 위한 필수 기술인 잔차 연결(Residual Connection)과 층 정규화(Layer Normalization) 적용.
- **객체지향적 모듈화 설계**: 프로젝트를 `config`, `dataset`, `model`, `train`, `generate` 스크립트로 역할을 분리하여, 재사용 가능하고 유지보수가 용이한 구조로 설계했습니다.
- **사전 훈련된 토크나이저 활용**: Hugging Face의 `transformers` 라이브러리를 사용하여, 대규모 한국어 데이터로 미리 훈련된 고성능 BPE 토크나이저를 모델에 통합했습니다.

---

## 📂 프로젝트 구조

```bash
Transformer/
├── models/               # 훈련된 모델 가중치(.pth) 저장소
│
├── config.py             # ⚙️ 모든 하이퍼파라미터 및 경로 설정
├── dataset.py            # ⚙️ 데이터 로딩 및 배치 생성
├── model.py              # ⚙️ 트랜스포머 아키텍처 (핵심 로직)
├── train.py              # ⚙️ 모델을 훈련시키는 메인 스크립트
└── generate.py           # ✍️ 훈련된 모델로 텍스트를 생성하는 스크립트
```

---

## 🚀 사용 방법

### 1. 환경 설정

```bash
# 필요한 라이브러리 설치
pip install torch transformers datasets tqdm
```

### 2. 모델 훈련

`config.py` 파일에서 원하는 하이퍼파라미터를 설정한 뒤, 아래 명령어를 실행하여 트랜스포머 모델을 훈련시킵니다.  
훈련이 완료되면 `models/` 폴더에 `transformer_model.pth` 파일이 저장됩니다.

```bash
python train.py
```

### 3. 텍스트 생성

훈련된 모델을 사용하여 새로운 텍스트를 생성합니다.

```bash
python generate.py
```

스크립트가 실행되면 프롬프트를 입력하라는 메시지가 나타납니다.

---

## 🎓 느낀점

이 프로젝트를 통해 언어 모델이 단순히 코드 그 자체를 넘어 수많은 작은 아이디어와 수학적 원리가 결합된 정교한 시스템이라는 것을 체감할 수 있었습니다.  

특히, 모든 토큰이 서로의 관계를 계산하는 셀프 어텐션의 원리를 코드로 직접 구현하며 소름이 돋는 것을 느꼈고, 모델의 안정성을 위해 층 정규화와 잔차 연결이 왜 필수적인지 몸소 깨달았습니다.  

또한, 단순한 문자 단위에서 고성능 BPE 토크나이저로 전환하면서 **효율적인 데이터 전처리가 모델 성능에 얼마나 큰 영향을 미치는지** 확인하는 경험은 매우 인상 깊었습니다.  

